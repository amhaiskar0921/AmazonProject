{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/amhaiskar0921/AmazonProject/blob/main/main_clean.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import pyarrow.parquet as pq\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# For data visualization\n",
        "import seaborn as sns"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rjr3CeYeRMJv",
        "outputId": "4d148450-6ca1-4f43-867b-1b6ef1818ff3"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(42)\n",
        "\n",
        "sample_size = 10000\n",
        "\n",
        "shopping_data = pq.read_table('/content/drive/MyDrive/Amazon (LA) - Multi-Class Product Classification (Team A)/Datasets/shopping_queries_dataset_examples.parquet')\n",
        "df = shopping_data.to_pandas().sample(n=sample_size, random_state=42)\n",
        "shopping_data_p = pq.read_table('/content/drive/MyDrive/Amazon (LA) - Multi-Class Product Classification (Team A)/Datasets/shopping_queries_dataset_products.parquet')\n",
        "df_p = shopping_data_p.to_pandas() #no sampling to match more shopping data\n",
        "df_p = df_p.drop(columns=['product_locale']) #this is a duplicate from shopping_data\n",
        "\n",
        "# merging\n",
        "df_merged = pd.merge(df, df_p, on='product_id', how='inner')\n",
        "print(f\"number of products {len(pd.unique(df_merged['product_id']))}\")\n",
        "df_merged_no_null = df_merged.fillna(\"\")\n",
        "missing_values = df_merged_no_null.isnull().sum()\n",
        "print(len(df_merged_no_null))"
      ],
      "metadata": {
        "id": "5ihO4pr28J5Z",
        "outputId": "99543495-f81d-4864-d0e0-2b661a9ebd69",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "number of products 9963\n",
            "10220\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# concat dat\n",
        "!pip install transformers\n",
        "import torch\n",
        "import transformers\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "from transformers import BertTokenizer, BertModel, AdamW, get_linear_schedule_with_warmup, AutoModelForSequenceClassification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "5OVuXpmF0nmY",
        "outputId": "0191c6ec-abfe-4a7a-bd7a-c9c85d064c7a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers\n",
            "  Downloading transformers-4.34.1-py3-none-any.whl (7.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.7/7.7 MB\u001b[0m \u001b[31m48.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.4)\n",
            "Collecting huggingface-hub<1.0,>=0.16.4 (from transformers)\n",
            "  Downloading huggingface_hub-0.18.0-py3-none-any.whl (301 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.0/302.0 kB\u001b[0m \u001b[31m33.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Collecting tokenizers<0.15,>=0.14 (from transformers)\n",
            "  Downloading tokenizers-0.14.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m88.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting safetensors>=0.3.1 (from transformers)\n",
            "  Downloading safetensors-0.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m68.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.5.0)\n",
            "Collecting huggingface-hub<1.0,>=0.16.4 (from transformers)\n",
            "  Downloading huggingface_hub-0.17.3-py3-none-any.whl (295 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m295.0/295.0 kB\u001b[0m \u001b[31m28.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n",
            "Installing collected packages: safetensors, huggingface-hub, tokenizers, transformers\n",
            "Successfully installed huggingface-hub-0.17.3 safetensors-0.4.0 tokenizers-0.14.1 transformers-4.34.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords"
      ],
      "metadata": {
        "id": "eJJi90Eh7lSt",
        "outputId": "5bb06394-d6f7-4053-c663-6efee54014fb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Add this code before tokenization\n",
        "def remove_stopwords(text):\n",
        "    stop_words_english = set(stopwords.words('english'))\n",
        "    stop_words_spanish = set(stopwords.words('spanish'))\n",
        "    # stop_words_japanese = set(stopwords.words('japanese')) <-- resulted in error\n",
        "    words = text.split()\n",
        "    filtered_words = [word for word in words if word.lower() not in stop_words_english\n",
        "                      and word.lower() not in stop_words_spanish]\n",
        "    return ' '.join(filtered_words)"
      ],
      "metadata": {
        "id": "TOOPn7-X79VS"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get a random sample of 10000 elems in the merged dataset with null values replaced\n",
        "sample_size = 10000\n",
        "\n",
        "# Initialize an empty list to store the modified input sequences\n",
        "input_sequences = []\n",
        "\n",
        "# Apply stopword removal to each element in the 'input_sequences' list using a for loop\n",
        "for text in df_merged_no_null.apply(lambda x: f\"[CLS] {x['query']} [SEP] {x['product_title']} [SEP] {x['product_description']} [SEP] {x['product_bullet_point']} [SEP] {x['product_color']} [SEP] {x['product_bullet_point']} [SEP]\", axis=1).tolist():\n",
        "# for text in df_merged_no_null.apply(lambda x: f\"[CLS] {x['product_title']} [SEP] {x['product_description']} [SEP] {x['product_bullet_point']} [SEP]\", axis=1).tolist():\n",
        "    cleaned_text = remove_stopwords(text)\n",
        "    input_sequences.append(cleaned_text)\n",
        "print(input_sequences[0], len(input_sequences))"
      ],
      "metadata": {
        "id": "Y5lZQZLd9d-K",
        "outputId": "d7663913-e77b-48ad-f89e-5a10ea2490d1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CLS] 100% cotton long sleeve shirt women [SEP] Anvil Ladies' Lightweight Long-Sleeve T-Shirt, Green Apple, Medium [SEP] 4.5 oz., preshrunk 100% combed ringspun cotton; 30 singles; Heathers-60/40 poly/combed ringspun cotton; Heather Grey - 90/10 combed ringspun cotton/poly; shoulder-to-shoulder tape; double-needle bottom hem; double-needle sleeve; Oeko-Tex Standard 100 Certified; features TearAway label; missy semi-fitted contoured silhouette sideseams; seamed collar [SEP] 4.5 oz., preshrunk 100% combed ringspun cotton 30 singles Heathers-60/40 poly/combed ringspun cotton Heather Grey - 90/10 combed ringspun cotton/poly shoulder-to-shoulder tape [SEP] Green Apple [SEP] 4.5 oz., preshrunk 100% combed ringspun cotton 30 singles Heathers-60/40 poly/combed ringspun cotton Heather Grey - 90/10 combed ringspun cotton/poly shoulder-to-shoulder tape [SEP] 10220\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the BERT tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "# Example text to tokenize\n",
        "# text = \"This is an example sentence.\"\n",
        "\n",
        "# Tokenize the text\n",
        "tokens = tokenizer(input_sequences, padding=True, truncation=True, return_tensors=\"pt\")\n",
        "\n",
        "# Convert tokenized text to PyTorch tensors\n",
        "input_ids = tokens['input_ids']\n",
        "attention_mask = tokens['attention_mask']"
      ],
      "metadata": {
        "id": "BkHJCLQk-kZk"
      },
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "def get_data_loader(features, max_seq_length, batch_size, shuffle=True):\n",
        "\n",
        "    all_input_ids = torch.tensor([f.input_ids for f in features], dtype=torch.long)\n",
        "    all_input_mask = torch.tensor([f.input_mask for f in features], dtype=torch.long)\n",
        "    all_segment_ids = torch.tensor([f.segment_ids for f in features], dtype=torch.long)\n",
        "    all_label_ids = torch.tensor([f.label_id for f in features], dtype=torch.long)\n",
        "    data = TensorDataset(all_input_ids, all_input_mask, all_segment_ids, all_label_ids)\n",
        "\n",
        "    dataloader = DataLoader(data, shuffle=shuffle, batch_size=batch_size)\n",
        "    return dataloader\n",
        "class BertInputItem(object):\n",
        "  \"\"\"An item with all the necessary attributes for finetuning BERT.\"\"\"\n",
        "\n",
        "  def __init__(self, text, input_ids, input_mask, segment_ids, label_id):\n",
        "      self.text = text\n",
        "      self.input_ids = input_ids\n",
        "      self.input_mask = input_mask\n",
        "      self.segment_ids = segment_ids\n",
        "      self.label_id = label_id\n"
      ],
      "metadata": {
        "id": "6YGNCQu4DPPv"
      },
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def convert_examples_to_inputs(example_texts, example_labels, label2idx, max_seq_length, tokenizer, verbose=0):\n",
        "      \"\"\"Loads a data file into a list of `InputBatch`s.\"\"\"\n",
        "\n",
        "      input_items = []\n",
        "      examples = zip(example_texts, example_labels)\n",
        "      for (ex_index, (text, label)) in enumerate(examples):\n",
        "\n",
        "          # Create a list of token ids\n",
        "          input_ids = tokenizer.encode(f\"[CLS] {text} [SEP]\")\n",
        "          if len(input_ids) > max_seq_length:\n",
        "              input_ids = input_ids[:max_seq_length]\n",
        "\n",
        "          # All our tokens are in the first input segment (id 0).\n",
        "          segment_ids = [0] * len(input_ids)\n",
        "\n",
        "          # The mask has 1 for real tokens and 0 for padding tokens. Only real\n",
        "          # tokens are attended to.\n",
        "          input_mask = [1] * len(input_ids)\n",
        "\n",
        "          # Zero-pad up to the sequence length.\n",
        "          padding = [0] * (max_seq_length - len(input_ids))\n",
        "          input_ids += padding\n",
        "          input_mask += padding\n",
        "          segment_ids += padding\n",
        "\n",
        "          assert len(input_ids) == max_seq_length\n",
        "          assert len(input_mask) == max_seq_length\n",
        "          assert len(segment_ids) == max_seq_length\n",
        "\n",
        "          label_id = label2idx[label]\n",
        "\n",
        "          input_items.append(\n",
        "              BertInputItem(text=text,\n",
        "                            input_ids=input_ids,\n",
        "                            input_mask=input_mask,\n",
        "                            segment_ids=segment_ids,\n",
        "                            label_id=label_id))\n",
        "\n",
        "\n",
        "      return input_items"
      ],
      "metadata": {
        "id": "Rbl3mmrWA2YQ"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rest_texts, test_texts, rest_labels, test_labels = train_test_split(input_sequences, df_merged_no_null['esci_label'].tolist(), test_size=0.1, random_state=1)\n",
        "train_texts, dev_texts, train_labels, dev_labels = train_test_split(rest_texts, rest_labels, test_size=0.1, random_state=1)\n",
        "\n",
        "target_names = list(set(df_merged_no_null['esci_label'].tolist()))\n",
        "label2idx = {label: idx for idx, label in enumerate(target_names)}\n",
        "print(label2idx)"
      ],
      "metadata": {
        "id": "SIz5qNzOBDhJ",
        "outputId": "2da676da-e3be-4965-dda3-d0196b2c3f6f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'C': 0, 'S': 1, 'E': 2, 'I': 3}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# inputItem = BertInputItem()\n",
        "MAX_SEQ_LENGTH = 512\n",
        "train_features = convert_examples_to_inputs(train_texts, train_labels, label2idx, MAX_SEQ_LENGTH, tokenizer, verbose=0)\n",
        "dev_features = convert_examples_to_inputs(dev_texts, dev_labels, label2idx, MAX_SEQ_LENGTH, tokenizer)\n",
        "test_features = convert_examples_to_inputs(test_texts, test_labels, label2idx, MAX_SEQ_LENGTH, tokenizer)\n",
        "\n",
        "BATCH_SIZE = 16\n",
        "\n",
        "train_dataloader = get_data_loader(train_features, MAX_SEQ_LENGTH, BATCH_SIZE, shuffle=True)\n",
        "dev_dataloader = get_data_loader(dev_features, MAX_SEQ_LENGTH, BATCH_SIZE, shuffle=False)\n",
        "test_dataloader = get_data_loader(test_features, MAX_SEQ_LENGTH, BATCH_SIZE, shuffle=False)"
      ],
      "metadata": {
        "id": "3adi06gV_JxY",
        "outputId": "0ddec020-fa09-4529-9ece-1880280b367b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Token indices sequence length is longer than the specified maximum sequence length for this model (808 > 512). Running this sequence through the model will result in indexing errors\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_merged_no_null.shape"
      ],
      "metadata": {
        "id": "0e2pIdQjMG56",
        "outputId": "ce3304e3-9861-4411-fe56-7bb56b4001a1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10220, 14)"
            ]
          },
          "metadata": {},
          "execution_count": 79
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# creating custom model with numerical features\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-cased\")\n",
        "additional_features = df_merged_no_null[[\"large_version\", \"small_version\"]] #should we include this in the model?\n",
        "\n",
        "#work in progress, need to create base model\n",
        "import torch.optim as optim\n",
        "from torch import nn\n",
        "\n",
        "EPOCHS = 1\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(EPOCHS):\n",
        "    model.train()\n",
        "    for batch in train_dataloader:\n",
        "        text_inputs, numerical_inputs, labels = batch\n",
        "\n",
        "        # Clear gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(text_inputs, numerical_inputs)\n",
        "\n",
        "        # Calculate loss\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # Backpropagation\n",
        "        loss.backward()\n",
        "        optimizer.step()\n"
      ],
      "metadata": {
        "id": "-Vf7-H0gMPNp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gUkw2YOfVTrG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}