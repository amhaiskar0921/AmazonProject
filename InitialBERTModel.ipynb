{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNUL7B9H6Ayq6VrWsg67ELT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/amhaiskar0921/AmazonProject/blob/main/BERT_Based_Multilingual.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Goal of this notebook: get a BERT model working"
      ],
      "metadata": {
        "id": "rZYBdURirFbo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Loading + merging the datasets"
      ],
      "metadata": {
        "id": "sOdsYwaDrRFG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import pyarrow.parquet as pq\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# For data visualization\n",
        "import seaborn as sns"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wL1VG-O8ehZC",
        "outputId": "1497f2fe-53a9-4669-f1f9-a85bd73f0743"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Getting random samples of our raw datasets\n",
        "np.random.seed(42)\n",
        "\n",
        "sample_size = 10000\n",
        "\n",
        "shopping_data = pq.read_table('/content/drive/MyDrive/Amazon (LA) - Multi-Class Product Classification (Team A)/Datasets/shopping_queries_dataset_examples.parquet')\n",
        "df_examples = shopping_data.to_pandas().sample(n=sample_size, random_state=42)\n",
        "\n",
        "# This line takes up 7GB ram and a couple of seconds to run\n",
        "shopping_data_p = pq.read_table('/content/drive/MyDrive/Amazon (LA) - Multi-Class Product Classification (Team A)/Datasets/shopping_queries_dataset_products.parquet')\n",
        "df_products = shopping_data_p.to_pandas().sample(n=sample_size, random_state=42)"
      ],
      "metadata": {
        "id": "gTLqQcSweoU9"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# SAVING RAM BY NOT MAKING COPIES OF EXISTING DATAFRAMES\n",
        "# Creating the merged dataset, null values replaced with empty strings\n",
        "# Taken from the esci challenge GitHub notebook\n",
        "df_examples_products = pd.merge(\n",
        "    df_examples,\n",
        "    df_products,\n",
        "    how='left',\n",
        "    left_on=['product_locale','product_id'],\n",
        "    right_on=['product_locale', 'product_id'],\n",
        "    copy=False\n",
        ")\n",
        "\n",
        "# Replacing null values with \"\"\n",
        "df_examples_products.fillna(\"\", inplace=True)\n",
        "\n",
        "# df_examples_products.head(20)"
      ],
      "metadata": {
        "id": "dScEk69Je7EB"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_examples_products.drop(df_examples_products[df_examples_products.large_version == 0].index, inplace = True)\n",
        "df_examples_products_train = df_examples_products[df_examples_products[\"split\"] == \"train\"]\n",
        "df_examples_products_test = df_examples_products[df_examples_products[\"split\"] == \"test\"]\n",
        "df_examples_products.head(10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 712
        },
        "id": "3XSn2bK_g93H",
        "outputId": "b9f10588-cac9-42bb-e6ee-90e27d8d1239"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   example_id                                  query  query_id  product_id  \\\n",
              "0       40527  100% cotton long sleeve t shirt women      1450  B014WD3WOG   \n",
              "1     1351394                  mi full screen tv pro     68591  B07Y25LMNP   \n",
              "2      895892                               geocache     44920  B014I55V8S   \n",
              "3     2454618                      メンズ t シャツ バックプリント    124137  B086DHGYGF   \n",
              "4     1302108            mary kay makeup remover eye     66054  B017PCGABI   \n",
              "5     2361724                                    シーツ    120029  B01M0IJYH4   \n",
              "6      870918             furniture no scratch spray     43627  B009ETNUJ6   \n",
              "7     1718789                              rifle bag     87640  B0897157XP   \n",
              "8     1710062               resin circular sandpaper     87199  B000022339   \n",
              "9     1684868                 rca 2 male to 1 female     85882  B095SBFQLZ   \n",
              "\n",
              "  product_locale esci_label  small_version  large_version  split  \\\n",
              "0             us          E              1              1   test   \n",
              "1             es          I              1              1  train   \n",
              "2             us          E              0              1  train   \n",
              "3             jp          E              0              1  train   \n",
              "4             us          S              1              1  train   \n",
              "5             jp          E              0              1  train   \n",
              "6             us          E              1              1  train   \n",
              "7             us          E              0              1  train   \n",
              "8             us          E              1              1  train   \n",
              "9             us          E              1              1  train   \n",
              "\n",
              "  product_title product_description product_bullet_point product_brand  \\\n",
              "0                                                                        \n",
              "1                                                                        \n",
              "2                                                                        \n",
              "3                                                                        \n",
              "4                                                                        \n",
              "5                                                                        \n",
              "6                                                                        \n",
              "7                                                                        \n",
              "8                                                                        \n",
              "9                                                                        \n",
              "\n",
              "  product_color  \n",
              "0                \n",
              "1                \n",
              "2                \n",
              "3                \n",
              "4                \n",
              "5                \n",
              "6                \n",
              "7                \n",
              "8                \n",
              "9                "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-6f7883f1-de6a-4599-acfc-63b8b0d62bea\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>example_id</th>\n",
              "      <th>query</th>\n",
              "      <th>query_id</th>\n",
              "      <th>product_id</th>\n",
              "      <th>product_locale</th>\n",
              "      <th>esci_label</th>\n",
              "      <th>small_version</th>\n",
              "      <th>large_version</th>\n",
              "      <th>split</th>\n",
              "      <th>product_title</th>\n",
              "      <th>product_description</th>\n",
              "      <th>product_bullet_point</th>\n",
              "      <th>product_brand</th>\n",
              "      <th>product_color</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>40527</td>\n",
              "      <td>100% cotton long sleeve t shirt women</td>\n",
              "      <td>1450</td>\n",
              "      <td>B014WD3WOG</td>\n",
              "      <td>us</td>\n",
              "      <td>E</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>test</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1351394</td>\n",
              "      <td>mi full screen tv pro</td>\n",
              "      <td>68591</td>\n",
              "      <td>B07Y25LMNP</td>\n",
              "      <td>es</td>\n",
              "      <td>I</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>train</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>895892</td>\n",
              "      <td>geocache</td>\n",
              "      <td>44920</td>\n",
              "      <td>B014I55V8S</td>\n",
              "      <td>us</td>\n",
              "      <td>E</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>train</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2454618</td>\n",
              "      <td>メンズ t シャツ バックプリント</td>\n",
              "      <td>124137</td>\n",
              "      <td>B086DHGYGF</td>\n",
              "      <td>jp</td>\n",
              "      <td>E</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>train</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1302108</td>\n",
              "      <td>mary kay makeup remover eye</td>\n",
              "      <td>66054</td>\n",
              "      <td>B017PCGABI</td>\n",
              "      <td>us</td>\n",
              "      <td>S</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>train</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>2361724</td>\n",
              "      <td>シーツ</td>\n",
              "      <td>120029</td>\n",
              "      <td>B01M0IJYH4</td>\n",
              "      <td>jp</td>\n",
              "      <td>E</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>train</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>870918</td>\n",
              "      <td>furniture no scratch spray</td>\n",
              "      <td>43627</td>\n",
              "      <td>B009ETNUJ6</td>\n",
              "      <td>us</td>\n",
              "      <td>E</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>train</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>1718789</td>\n",
              "      <td>rifle bag</td>\n",
              "      <td>87640</td>\n",
              "      <td>B0897157XP</td>\n",
              "      <td>us</td>\n",
              "      <td>E</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>train</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>1710062</td>\n",
              "      <td>resin circular sandpaper</td>\n",
              "      <td>87199</td>\n",
              "      <td>B000022339</td>\n",
              "      <td>us</td>\n",
              "      <td>E</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>train</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>1684868</td>\n",
              "      <td>rca 2 male to 1 female</td>\n",
              "      <td>85882</td>\n",
              "      <td>B095SBFQLZ</td>\n",
              "      <td>us</td>\n",
              "      <td>E</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>train</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-6f7883f1-de6a-4599-acfc-63b8b0d62bea')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-6f7883f1-de6a-4599-acfc-63b8b0d62bea button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-6f7883f1-de6a-4599-acfc-63b8b0d62bea');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-b65dab90-2c73-4e4a-8b38-8f9e0ef3c1ed\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-b65dab90-2c73-4e4a-8b38-8f9e0ef3c1ed')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-b65dab90-2c73-4e4a-8b38-8f9e0ef3c1ed button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Preprocessing the data"
      ],
      "metadata": {
        "id": "VP21wJy2rBsv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Getting all the stopwords"
      ],
      "metadata": {
        "id": "F3HlSZeHrYRz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xo037_RDkaAy",
        "outputId": "07e81a02-82c6-4200-a009-15a281a1aa95"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Getting Japanese stopwords from this [GitHub repo](https://github.com/stopwords-iso/stopwords-ja)\n",
        "\n",
        "Inspo from: https://stackoverflow.com/questions/72149806/exclude-japanese-stopwords-from-file"
      ],
      "metadata": {
        "id": "dSRQ1-4Tr18Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import urllib\n",
        "from urllib.request import urlopen\n",
        "\n",
        "def get_japanese_stopwords():\n",
        "  iso_path = \"https://raw.githubusercontent.com/stopwords-iso/stopwords-ja/master/stopwords-ja.txt\"\n",
        "  iso_file = urllib.request.urlopen(iso_path)\n",
        "  stopwords = [line.decode(\"utf-8\").strip() for line in iso_file]\n",
        "\n",
        "  stopwords = [ss for ss in stopwords if not ss==u'']\n",
        "  stopwords = set(stopwords)\n",
        "  return stopwords\n"
      ],
      "metadata": {
        "id": "TTknFz5trtrh"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "NLTK has English and Spanish stopwords"
      ],
      "metadata": {
        "id": "AXGfv5_KstRp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_all_stopwords():\n",
        "  stop_words_english = set(stopwords.words('english'))\n",
        "  # Merging english and spanish\n",
        "  stop_words_english_and_spanish = stop_words_english.union(set(stopwords.words('spanish')))\n",
        "  # Merging japanese with english and spanish\n",
        "  all_stopwords = stop_words_english_and_spanish.union(get_japanese_stopwords())\n",
        "  # Returning a set of the all stopwords\n",
        "  return all_stopwords"
      ],
      "metadata": {
        "id": "yJjmpYNUsiJL"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load NLTK stopwords, stemmer, and lemmatizer\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "nltk.download('wordnet')\n",
        "\n",
        "all_stopwords = get_all_stopwords()\n",
        "porter_stemmer = PorterStemmer()\n",
        "wordnet_lemmatizer = WordNetLemmatizer()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o8YRcUsjsq02",
        "outputId": "11f7728d-c42d-469d-c470-7ddf91da1e41"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Stemming, lemmatizing"
      ],
      "metadata": {
        "id": "uP_pu9JnwAeU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to preprocess text\n",
        "def preprocess_text(text):\n",
        "    # Tokenize text\n",
        "    tokens = tokenizer.tokenize(tokenizer.decode(tokenizer.encode(text)))\n",
        "\n",
        "    # Remove stopwords, apply stemming and lemmatization\n",
        "    tokens = [porter_stemmer.stem(token) for token in tokens if token.lower() not in all_stopwords]\n",
        "    tokens = [wordnet_lemmatizer.lemmatize(token) for token in tokens if token.lower() not in all_stopwords]\n",
        "\n",
        "    # Concatenate tokens into a single string\n",
        "    processed_text = ' '.join(tokens)\n",
        "\n",
        "    return processed_text"
      ],
      "metadata": {
        "id": "Ro65OBrIs-Mm"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers\n",
        "from transformers import BertTokenizer, TFAutoModelForSequenceClassification"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S9OZmXxyt1wy",
        "outputId": "2c7b30bd-bbd9-40d8-edd0-bee8aa99d0f3"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.35.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.0)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the BERT tokenizer to get individual words\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')"
      ],
      "metadata": {
        "id": "oQbpPGnLuJsT"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Full preprocessing function"
      ],
      "metadata": {
        "id": "ng4Td9G9xTKX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Applying the above preprocessing function to train and test data"
      ],
      "metadata": {
        "id": "fSwijJG9wPWP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Applying the preprocessing function to the product description and bullet point columns"
      ],
      "metadata": {
        "id": "JCi9a5oxxcg-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_examples_products_train['product_description'] = df_examples_products_train['product_description'].apply(preprocess_text)\n",
        "df_examples_products_test['product_bullet_point'] = df_examples_products_test['product_bullet_point'].apply(preprocess_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mwFWslynwsCu",
        "outputId": "a175f2f5-11e3-4346-dd25-34096bbd0947"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Token indices sequence length is longer than the specified maximum sequence length for this model (624 > 512). Running this sequence through the model will result in indexing errors\n",
            "<ipython-input-12-2b695353d7fe>:1: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df_examples_products_train['product_description'] = df_examples_products_train['product_description'].apply(preprocess_text)\n",
            "<ipython-input-12-2b695353d7fe>:2: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df_examples_products_test['product_bullet_point'] = df_examples_products_test['product_bullet_point'].apply(preprocess_text)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creating input sequences for the BERT model\n",
        "Columns included: query, desc, bullet points, title, brand"
      ],
      "metadata": {
        "id": "K7hfUvVfyOHn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create input sequences for the BERT model\n",
        "def create_input_sequence(row: 'DataFrame_row'):\n",
        "    # Concatenate relevant columns: query, desc, bullet points, title, brand\n",
        "    input_text = f\"[CLS] {row['query']} [SEP] {row['product_description']} [SEP] {row['product_bullet_point']} [SEP] {row['product_title']} [SEP] {row['product_brand']} [SEP]\"\n",
        "\n",
        "    # Tokenize and encode the input\n",
        "    encoded_input = tokenizer(input_text, padding=\"max_length\", truncation=True, return_tensors='tf')\n",
        "\n",
        "    return encoded_input\n"
      ],
      "metadata": {
        "id": "-F-bVevEyS3_"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf"
      ],
      "metadata": {
        "id": "iN7mA3NDBoCQ"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating input sequences for train and test data\n",
        "df_examples_products_train['bert_input'] = df_examples_products_train.apply(create_input_sequence, axis=1)\n",
        "df_examples_products_test['bert_input'] = df_examples_products_test.apply(create_input_sequence, axis=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fz9UxXlwyJde",
        "outputId": "c6ed8286-1fec-4343-9b97-de2e5176888c"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-15-d283b22da496>:2: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df_examples_products_train['bert_input'] = df_examples_products_train.apply(create_input_sequence, axis=1)\n",
            "<ipython-input-15-d283b22da496>:3: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df_examples_products_test['bert_input'] = df_examples_products_test.apply(create_input_sequence, axis=1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(df_examples_products_train['bert_input'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m0PxnOrQ_UQ7",
        "outputId": "6610c527-a2b6-4159-b539-d624e2843568"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1       [input_ids, token_type_ids, attention_mask]\n",
            "2       [input_ids, token_type_ids, attention_mask]\n",
            "3       [input_ids, token_type_ids, attention_mask]\n",
            "4       [input_ids, token_type_ids, attention_mask]\n",
            "5       [input_ids, token_type_ids, attention_mask]\n",
            "                           ...                     \n",
            "9994    [input_ids, token_type_ids, attention_mask]\n",
            "9995    [input_ids, token_type_ids, attention_mask]\n",
            "9996    [input_ids, token_type_ids, attention_mask]\n",
            "9998    [input_ids, token_type_ids, attention_mask]\n",
            "9999    [input_ids, token_type_ids, attention_mask]\n",
            "Name: bert_input, Length: 7512, dtype: object\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Suggested by ChatGPT (RUNNING INTO ERRORS, DON'T RUN THIS):\n",
        "df_examples_products_train['bert_input'] = df_examples_products_train['bert_input'].apply(\n",
        "    lambda x: {\n",
        "        'input_ids': tf.concat([item['input_ids'] for item in x], axis=0),\n",
        "        'token_type_ids': tf.concat([item['token_type_ids'] for item in x], axis=0),\n",
        "        'attention_mask': tf.concat([item['attention_mask'] for item in x], axis=0),\n",
        "    }\n",
        ")\n",
        "\n",
        "df_examples_products_test['bert_input'] = df_examples_products_test['bert_input'].apply(\n",
        "    lambda x: {\n",
        "        'input_ids': tf.concat([item['input_ids'] for item in x], axis=0),\n",
        "        'token_type_ids': tf.concat([item['token_type_ids'] for item in x], axis=0),\n",
        "        'attention_mask': tf.concat([item['attention_mask'] for item in x], axis=0),\n",
        "    }\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 478
        },
        "id": "a8XvckQCCayD",
        "outputId": "f9c5190e-6bfa-4d8d-f2f6-8cd46ef30cc7"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-4f30c300a213>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Suggested by ChatGPT:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m df_examples_products_train['bert_input'] = df_examples_products_train['bert_input'].apply(\n\u001b[0m\u001b[1;32m      3\u001b[0m     lambda x: {\n\u001b[1;32m      4\u001b[0m         \u001b[0;34m'input_ids'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'input_ids'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0;34m'token_type_ids'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'token_type_ids'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/series.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, func, convert_dtype, args, **kwargs)\u001b[0m\n\u001b[1;32m   4769\u001b[0m         \u001b[0mdtype\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfloat64\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4770\u001b[0m         \"\"\"\n\u001b[0;32m-> 4771\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mSeriesApply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert_dtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4772\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4773\u001b[0m     def _reduce(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/apply.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1122\u001b[0m         \u001b[0;31m# self.f is Callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1123\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_standard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1124\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1125\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0magg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/apply.py\u001b[0m in \u001b[0;36mapply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1172\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1173\u001b[0m                 \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1174\u001b[0;31m                 mapped = lib.map_infer(\n\u001b[0m\u001b[1;32m   1175\u001b[0m                     \u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1176\u001b[0m                     \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/_libs/lib.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m<ipython-input-16-4f30c300a213>\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m      2\u001b[0m df_examples_products_train['bert_input'] = df_examples_products_train['bert_input'].apply(\n\u001b[1;32m      3\u001b[0m     lambda x: {\n\u001b[0;32m----> 4\u001b[0;31m         \u001b[0;34m'input_ids'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'input_ids'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m         \u001b[0;34m'token_type_ids'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'token_type_ids'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0;34m'attention_mask'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'attention_mask'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-16-4f30c300a213>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      2\u001b[0m df_examples_products_train['bert_input'] = df_examples_products_train['bert_input'].apply(\n\u001b[1;32m      3\u001b[0m     lambda x: {\n\u001b[0;32m----> 4\u001b[0;31m         \u001b[0;34m'input_ids'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'input_ids'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m         \u001b[0;34m'token_type_ids'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'token_type_ids'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0;34m'attention_mask'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'attention_mask'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: string indices must be integers"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Training the model"
      ],
      "metadata": {
        "id": "YdXv7UQI0SVw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Referring to: https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/text_classification-tf.ipynb#scrollTo=uJnpEhTw2GEf"
      ],
      "metadata": {
        "id": "YPs9VD9TKbw9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# labels\n",
        "id2label = {0: \"E\", 1: \"S\", 2: \"C\", 3: \"I\"}\n",
        "label2id = {val: key for key, val in id2label.items()}"
      ],
      "metadata": {
        "id": "2hoW1XZc0YFT"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Other model params\n",
        "MODEL_CHECKPOINT = \"bert-base-multilingual-cased\"\n",
        "BATCH_SIZE = 16\n",
        "NUM_LABELS = 4"
      ],
      "metadata": {
        "id": "P4hlBNEo3Fxt"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading the model\n",
        "model = TFAutoModelForSequenceClassification.from_pretrained(\n",
        "    MODEL_CHECKPOINT, num_labels=NUM_LABELS, id2label=id2label, label2id=label2id\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9zeBIxx42z_U",
        "outputId": "42cd9023-d9ac-4082-f74b-f12b5b9eb6c9"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "All PyTorch model weights were used when initializing TFBertForSequenceClassification.\n",
            "\n",
            "Some weights or buffers of the TF 2.0 model TFBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Getting the optimizer\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.optimizers import Adam as Adam"
      ],
      "metadata": {
        "id": "kdilS7l23e0s"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compile the model\n",
        "model.compile(optimizer=Adam(learning_rate=2e-5), loss='categorical_crossentropy')"
      ],
      "metadata": {
        "id": "6EKLmW8n56-s"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "# Initialize the tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-multilingual-cased\")\n",
        "\n",
        "def format_example(row):\n",
        "    # Combine text from the desired columns\n",
        "    text_to_tokenize = row['product_title'] + \" \" + row['product_description']\n",
        "\n",
        "    # Tokenize the text\n",
        "    tokens = tokenizer(text_to_tokenize, truncation=True, padding='max_length', max_length=128)\n",
        "\n",
        "    # Return the tokenized data with the label\n",
        "    return ({'input_ids': tokens['input_ids'],\n",
        "             'token_type_ids': tokens['token_type_ids'],\n",
        "             'attention_mask': tokens['attention_mask']}, row['esci_label'])\n",
        "\n",
        "# Creating a TensorFlow dataset from DataFrame\n",
        "train_data = tf.data.Dataset.from_tensor_slices((df_examples_products_train.to_dict('records')))\n",
        "train_data = train_data.map(format_example)\n",
        "\n",
        "# Batch the data\n",
        "train_data = train_data.batch(BATCH_SIZE)\n",
        "\n",
        "# Train the model\n",
        "model.fit(train_data, epochs=10)"
      ],
      "metadata": {
        "id": "TBAuNAYr59ec"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming 'bert_input' is a column containing dictionaries with 'input_ids', 'token_type_ids', and 'attention_mask'\n",
        "test_data = df_examples_products_test['bert_input'].tolist()\n",
        "\n",
        "# Assuming your model expects 'input_ids', 'token_type_ids', and 'attention_mask'\n",
        "# Concatenate them to create the input tensor\n",
        "input_ids = np.concatenate([item['input_ids'] for item in test_data], axis=0)\n",
        "token_type_ids = np.concatenate([item['token_type_ids'] for item in test_data], axis=0)\n",
        "attention_mask = np.concatenate([item['attention_mask'] for item in test_data], axis=0)\n",
        "\n",
        "# Reshape the arrays if needed (e.g., for models expecting batches)\n",
        "input_ids = np.reshape(input_ids, (-1, 512))\n",
        "token_type_ids = np.reshape(token_type_ids, (-1, 512))\n",
        "attention_mask = np.reshape(attention_mask, (-1, 512))\n",
        "\n",
        "# Make predictions\n",
        "predicted_labels = model.predict({'input_ids': input_ids, 'token_type_ids': token_type_ids, 'attention_mask': attention_mask})\n",
        "\n",
        "print(predicted_labels)\n",
        "\n",
        "# Print shapes for debugging\n",
        "print(\"True Labels Shape:\", len(df_examples_products_test['esci_label']))\n",
        "print(\"Predicted Labels Shape:\", len(predicted_labels))\n",
        "\n",
        "# Assuming predicted_labels is a numpy array with predicted labels\n",
        "# If your labels are one-hot encoded, use argmax to get the predicted class\n",
        "# predicted_labels = np.argmax(predicted_labels, axis=1)\n",
        "\n",
        "# Calculate micro average F1 score\n",
        "# micro_avg_f1 = f1_score(df_examples_products_test['esci_label'], predicted_labels, average='micro')\n",
        "\n",
        "# print(f\"Micro Average F1 Score: {micro_avg_f1}\")\n"
      ],
      "metadata": {
        "id": "giGF-R6zkiet",
        "outputId": "3d0e1de6-40f0-453e-b063-387424ec8778",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "78/78 [==============================] - 114s 1s/step\n",
            "TFSequenceClassifierOutput(loss=None, logits=array([[-0.09580151, -0.09682515, -0.01845061, -0.03488871],\n",
            "       [-0.10015909, -0.09560551, -0.0199514 , -0.06723654],\n",
            "       [-0.09870557, -0.04610171, -0.01216526, -0.05456041],\n",
            "       ...,\n",
            "       [-0.12025939, -0.1240601 ,  0.00450623, -0.10636111],\n",
            "       [-0.09062778, -0.10544816, -0.03217533, -0.07173488],\n",
            "       [-0.09513985, -0.09462745, -0.07373965, -0.06434902]],\n",
            "      dtype=float32), hidden_states=None, attentions=None)\n",
            "True Labels Shape: 2488\n",
            "Predicted Labels Shape: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "logits = predicted_labels.logits\n",
        "# Find the index of the maximum value along the last axis\n",
        "predicted_labels = np.argmax(logits, axis=-1)\n",
        "\n",
        "# Map the indices to your class labels\n",
        "class_labels = ['E', 'S', 'C', 'I']\n",
        "predicted_class_labels = [class_labels[i] for i in predicted_labels]\n",
        "\n",
        "# Print or use the predicted class labels\n",
        "print(predicted_class_labels)"
      ],
      "metadata": {
        "id": "2oCH16J_oi2O",
        "outputId": "d1869c0e-d5e7-4adf-fea0-f1861d099a4a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['C', 'C', 'C', 'S', 'I', 'C', 'I', 'I', 'I', 'I', 'I', 'C', 'C', 'I', 'C', 'C', 'I', 'C', 'I', 'S', 'C', 'C', 'I', 'C', 'I', 'I', 'S', 'S', 'C', 'C', 'I', 'I', 'C', 'C', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'C', 'C', 'C', 'C', 'C', 'C', 'I', 'E', 'I', 'C', 'I', 'C', 'I', 'C', 'I', 'I', 'I', 'C', 'I', 'C', 'C', 'I', 'I', 'I', 'S', 'C', 'C', 'C', 'I', 'E', 'I', 'C', 'C', 'I', 'S', 'I', 'C', 'S', 'I', 'C', 'I', 'C', 'I', 'I', 'C', 'C', 'I', 'E', 'I', 'I', 'S', 'I', 'I', 'I', 'C', 'C', 'I', 'C', 'C', 'I', 'C', 'C', 'C', 'I', 'I', 'I', 'I', 'C', 'I', 'C', 'S', 'I', 'C', 'C', 'I', 'I', 'I', 'I', 'C', 'I', 'S', 'I', 'I', 'C', 'I', 'I', 'C', 'S', 'I', 'C', 'C', 'E', 'I', 'C', 'C', 'I', 'C', 'I', 'C', 'I', 'C', 'C', 'C', 'E', 'E', 'C', 'C', 'I', 'C', 'C', 'C', 'C', 'I', 'C', 'I', 'C', 'I', 'E', 'C', 'I', 'I', 'I', 'C', 'S', 'C', 'I', 'C', 'C', 'I', 'I', 'I', 'I', 'C', 'C', 'C', 'C', 'I', 'C', 'I', 'C', 'I', 'C', 'I', 'I', 'E', 'I', 'C', 'C', 'I', 'C', 'C', 'C', 'C', 'C', 'C', 'C', 'C', 'C', 'C', 'I', 'C', 'I', 'I', 'C', 'I', 'C', 'C', 'C', 'C', 'E', 'I', 'I', 'C', 'I', 'C', 'I', 'C', 'S', 'C', 'I', 'I', 'E', 'C', 'C', 'I', 'C', 'I', 'C', 'I', 'C', 'C', 'I', 'I', 'C', 'I', 'I', 'I', 'I', 'I', 'S', 'I', 'I', 'C', 'I', 'I', 'I', 'C', 'S', 'I', 'I', 'C', 'C', 'I', 'C', 'S', 'I', 'C', 'C', 'I', 'C', 'C', 'I', 'I', 'C', 'C', 'S', 'C', 'I', 'C', 'I', 'I', 'I', 'C', 'E', 'S', 'C', 'C', 'C', 'C', 'I', 'I', 'C', 'I', 'C', 'S', 'C', 'I', 'I', 'C', 'E', 'E', 'I', 'I', 'C', 'C', 'C', 'C', 'I', 'C', 'C', 'C', 'C', 'C', 'I', 'C', 'I', 'C', 'I', 'C', 'I', 'I', 'I', 'I', 'C', 'C', 'I', 'C', 'C', 'I', 'I', 'C', 'I', 'I', 'C', 'C', 'C', 'C', 'I', 'C', 'C', 'C', 'I', 'C', 'I', 'I', 'I', 'E', 'C', 'I', 'C', 'I', 'I', 'C', 'C', 'C', 'C', 'I', 'C', 'C', 'C', 'C', 'I', 'I', 'I', 'I', 'I', 'C', 'C', 'C', 'I', 'C', 'I', 'C', 'I', 'C', 'S', 'I', 'C', 'C', 'I', 'I', 'I', 'I', 'C', 'I', 'C', 'C', 'C', 'C', 'C', 'I', 'C', 'C', 'C', 'C', 'S', 'I', 'I', 'I', 'C', 'I', 'I', 'C', 'C', 'C', 'I', 'I', 'C', 'C', 'I', 'C', 'I', 'I', 'C', 'C', 'C', 'I', 'C', 'C', 'I', 'I', 'C', 'C', 'C', 'I', 'C', 'I', 'I', 'I', 'C', 'I', 'C', 'I', 'C', 'E', 'I', 'I', 'I', 'C', 'C', 'I', 'I', 'I', 'I', 'C', 'I', 'I', 'C', 'S', 'I', 'C', 'I', 'C', 'C', 'C', 'I', 'C', 'C', 'I', 'C', 'C', 'C', 'I', 'C', 'I', 'I', 'I', 'C', 'I', 'S', 'C', 'C', 'C', 'C', 'I', 'S', 'I', 'I', 'C', 'E', 'I', 'I', 'I', 'I', 'C', 'C', 'C', 'I', 'C', 'C', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'C', 'I', 'C', 'C', 'C', 'C', 'C', 'C', 'C', 'C', 'I', 'I', 'C', 'C', 'C', 'C', 'I', 'E', 'I', 'I', 'S', 'C', 'I', 'I', 'C', 'C', 'C', 'I', 'I', 'I', 'C', 'I', 'I', 'I', 'C', 'C', 'I', 'C', 'C', 'C', 'I', 'I', 'I', 'C', 'I', 'I', 'C', 'E', 'I', 'C', 'I', 'I', 'I', 'C', 'C', 'C', 'C', 'C', 'C', 'I', 'C', 'C', 'I', 'I', 'C', 'I', 'C', 'C', 'C', 'I', 'C', 'C', 'I', 'C', 'I', 'I', 'S', 'C', 'I', 'C', 'I', 'C', 'I', 'I', 'I', 'C', 'I', 'I', 'C', 'C', 'C', 'I', 'I', 'I', 'C', 'I', 'C', 'C', 'I', 'C', 'I', 'I', 'I', 'I', 'I', 'C', 'C', 'C', 'I', 'C', 'I', 'S', 'C', 'I', 'I', 'C', 'C', 'C', 'C', 'C', 'C', 'I', 'C', 'C', 'I', 'S', 'I', 'I', 'C', 'C', 'C', 'I', 'I', 'I', 'C', 'I', 'I', 'C', 'I', 'C', 'C', 'I', 'I', 'C', 'C', 'I', 'C', 'I', 'C', 'I', 'I', 'C', 'C', 'E', 'C', 'I', 'I', 'E', 'C', 'C', 'I', 'I', 'I', 'C', 'I', 'I', 'C', 'I', 'I', 'I', 'C', 'S', 'C', 'C', 'C', 'I', 'C', 'I', 'I', 'C', 'I', 'C', 'I', 'C', 'C', 'I', 'I', 'C', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'E', 'C', 'C', 'S', 'C', 'C', 'C', 'C', 'C', 'I', 'C', 'C', 'C', 'I', 'I', 'I', 'I', 'S', 'S', 'E', 'C', 'C', 'I', 'I', 'E', 'C', 'I', 'I', 'C', 'I', 'C', 'C', 'I', 'I', 'I', 'I', 'I', 'C', 'C', 'C', 'S', 'I', 'C', 'I', 'C', 'I', 'I', 'S', 'C', 'I', 'C', 'C', 'I', 'C', 'I', 'S', 'I', 'I', 'I', 'I', 'C', 'S', 'C', 'C', 'I', 'C', 'C', 'C', 'I', 'C', 'C', 'I', 'I', 'C', 'E', 'C', 'I', 'C', 'I', 'C', 'I', 'I', 'C', 'C', 'E', 'C', 'C', 'I', 'C', 'C', 'C', 'C', 'I', 'I', 'C', 'I', 'C', 'C', 'I', 'I', 'C', 'C', 'C', 'I', 'C', 'C', 'C', 'I', 'C', 'C', 'I', 'C', 'C', 'I', 'I', 'C', 'I', 'E', 'C', 'I', 'I', 'C', 'I', 'C', 'I', 'I', 'I', 'C', 'I', 'C', 'C', 'I', 'I', 'E', 'I', 'I', 'E', 'C', 'I', 'C', 'I', 'I', 'C', 'C', 'C', 'I', 'C', 'C', 'C', 'C', 'C', 'I', 'C', 'C', 'C', 'I', 'C', 'C', 'C', 'C', 'C', 'C', 'C', 'I', 'C', 'S', 'C', 'I', 'I', 'S', 'I', 'I', 'C', 'I', 'I', 'C', 'I', 'C', 'C', 'C', 'C', 'S', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'C', 'I', 'I', 'C', 'I', 'I', 'C', 'S', 'C', 'I', 'C', 'I', 'I', 'C', 'I', 'I', 'I', 'I', 'I', 'C', 'I', 'I', 'C', 'S', 'C', 'I', 'C', 'C', 'E', 'C', 'C', 'I', 'C', 'C', 'I', 'E', 'C', 'I', 'I', 'S', 'C', 'C', 'I', 'S', 'C', 'C', 'C', 'C', 'C', 'S', 'C', 'I', 'C', 'C', 'I', 'C', 'I', 'I', 'I', 'I', 'C', 'I', 'I', 'C', 'I', 'I', 'C', 'C', 'I', 'S', 'C', 'E', 'C', 'C', 'I', 'I', 'I', 'C', 'C', 'I', 'I', 'I', 'C', 'I', 'C', 'E', 'I', 'C', 'I', 'I', 'C', 'I', 'I', 'I', 'I', 'I', 'I', 'C', 'I', 'I', 'C', 'C', 'C', 'C', 'I', 'I', 'E', 'I', 'C', 'I', 'C', 'I', 'E', 'C', 'I', 'I', 'I', 'I', 'I', 'S', 'I', 'E', 'I', 'C', 'I', 'I', 'I', 'C', 'I', 'I', 'I', 'C', 'C', 'I', 'C', 'I', 'C', 'E', 'I', 'I', 'C', 'C', 'S', 'C', 'I', 'S', 'I', 'C', 'I', 'I', 'I', 'C', 'C', 'I', 'C', 'I', 'C', 'C', 'I', 'C', 'I', 'S', 'I', 'I', 'I', 'I', 'C', 'I', 'C', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'C', 'C', 'C', 'S', 'I', 'C', 'I', 'I', 'I', 'I', 'I', 'C', 'C', 'I', 'I', 'E', 'C', 'C', 'C', 'C', 'C', 'C', 'E', 'C', 'I', 'I', 'I', 'C', 'I', 'C', 'I', 'C', 'C', 'C', 'I', 'C', 'C', 'C', 'I', 'C', 'C', 'E', 'C', 'C', 'C', 'S', 'E', 'C', 'I', 'I', 'I', 'S', 'E', 'S', 'I', 'C', 'I', 'I', 'C', 'I', 'E', 'C', 'C', 'C', 'I', 'I', 'E', 'I', 'I', 'C', 'C', 'I', 'C', 'I', 'C', 'C', 'S', 'I', 'C', 'I', 'I', 'C', 'E', 'S', 'I', 'I', 'I', 'C', 'I', 'C', 'I', 'C', 'I', 'C', 'I', 'I', 'C', 'I', 'C', 'I', 'I', 'E', 'C', 'I', 'I', 'S', 'C', 'C', 'C', 'C', 'S', 'I', 'C', 'C', 'I', 'C', 'C', 'I', 'I', 'C', 'C', 'I', 'C', 'C', 'C', 'S', 'I', 'I', 'S', 'S', 'I', 'I', 'C', 'C', 'I', 'S', 'C', 'C', 'C', 'I', 'C', 'I', 'C', 'C', 'I', 'C', 'I', 'C', 'C', 'C', 'C', 'I', 'C', 'C', 'C', 'I', 'C', 'I', 'S', 'I', 'I', 'C', 'I', 'C', 'C', 'C', 'C', 'C', 'E', 'I', 'I', 'E', 'C', 'I', 'S', 'C', 'C', 'C', 'C', 'I', 'C', 'C', 'C', 'C', 'I', 'I', 'I', 'I', 'I', 'I', 'C', 'C', 'C', 'E', 'C', 'I', 'C', 'E', 'C', 'I', 'C', 'I', 'C', 'I', 'E', 'I', 'I', 'C', 'I', 'S', 'I', 'C', 'I', 'C', 'E', 'I', 'I', 'I', 'C', 'C', 'I', 'S', 'C', 'C', 'S', 'C', 'C', 'C', 'C', 'C', 'I', 'I', 'I', 'I', 'I', 'C', 'C', 'E', 'C', 'I', 'E', 'I', 'C', 'I', 'C', 'C', 'C', 'I', 'I', 'S', 'C', 'I', 'E', 'C', 'I', 'S', 'C', 'C', 'I', 'I', 'C', 'C', 'C', 'C', 'C', 'I', 'I', 'I', 'I', 'C', 'I', 'C', 'I', 'I', 'C', 'E', 'C', 'I', 'I', 'I', 'C', 'C', 'I', 'S', 'C', 'C', 'C', 'I', 'C', 'I', 'I', 'C', 'I', 'I', 'E', 'E', 'I', 'C', 'I', 'C', 'C', 'I', 'C', 'I', 'C', 'E', 'C', 'I', 'I', 'C', 'I', 'I', 'C', 'C', 'C', 'C', 'C', 'C', 'S', 'C', 'I', 'C', 'C', 'I', 'I', 'C', 'I', 'I', 'C', 'C', 'I', 'C', 'C', 'I', 'I', 'I', 'C', 'C', 'C', 'E', 'I', 'I', 'I', 'C', 'C', 'C', 'E', 'C', 'I', 'I', 'C', 'C', 'C', 'C', 'C', 'C', 'I', 'I', 'I', 'C', 'C', 'C', 'I', 'C', 'I', 'C', 'C', 'I', 'E', 'I', 'I', 'I', 'I', 'I', 'C', 'C', 'I', 'C', 'C', 'C', 'I', 'C', 'I', 'C', 'C', 'C', 'C', 'I', 'I', 'I', 'C', 'I', 'I', 'E', 'C', 'I', 'I', 'C', 'I', 'I', 'C', 'I', 'C', 'I', 'S', 'C', 'C', 'I', 'C', 'C', 'E', 'I', 'C', 'I', 'C', 'I', 'E', 'C', 'I', 'I', 'C', 'C', 'C', 'C', 'I', 'S', 'C', 'C', 'C', 'S', 'I', 'C', 'C', 'C', 'C', 'I', 'C', 'E', 'C', 'C', 'I', 'C', 'I', 'C', 'C', 'I', 'C', 'I', 'E', 'I', 'C', 'I', 'I', 'C', 'I', 'I', 'C', 'C', 'E', 'I', 'S', 'S', 'C', 'I', 'C', 'C', 'I', 'I', 'C', 'I', 'I', 'S', 'C', 'I', 'C', 'C', 'I', 'I', 'C', 'E', 'I', 'E', 'I', 'I', 'I', 'C', 'I', 'C', 'C', 'I', 'C', 'C', 'I', 'C', 'S', 'S', 'C', 'S', 'C', 'E', 'S', 'I', 'C', 'C', 'I', 'C', 'I', 'C', 'C', 'C', 'C', 'I', 'I', 'C', 'C', 'I', 'C', 'I', 'C', 'C', 'E', 'C', 'I', 'C', 'C', 'I', 'I', 'C', 'I', 'I', 'I', 'I', 'C', 'C', 'S', 'C', 'C', 'I', 'I', 'C', 'C', 'I', 'I', 'C', 'I', 'C', 'I', 'C', 'C', 'I', 'C', 'I', 'C', 'C', 'I', 'I', 'S', 'C', 'E', 'I', 'C', 'C', 'S', 'C', 'C', 'I', 'I', 'C', 'I', 'I', 'S', 'I', 'I', 'I', 'S', 'C', 'C', 'C', 'I', 'I', 'I', 'I', 'C', 'I', 'C', 'I', 'C', 'C', 'I', 'C', 'C', 'C', 'I', 'C', 'I', 'E', 'C', 'I', 'I', 'I', 'I', 'C', 'C', 'C', 'C', 'C', 'I', 'I', 'I', 'C', 'C', 'C', 'I', 'C', 'C', 'E', 'S', 'C', 'C', 'I', 'C', 'C', 'I', 'I', 'I', 'I', 'C', 'C', 'I', 'I', 'C', 'S', 'S', 'C', 'C', 'I', 'I', 'I', 'C', 'I', 'I', 'I', 'I', 'I', 'I', 'C', 'S', 'I', 'I', 'I', 'C', 'C', 'C', 'I', 'C', 'C', 'C', 'I', 'I', 'I', 'I', 'I', 'C', 'C', 'C', 'C', 'C', 'C', 'C', 'I', 'C', 'C', 'C', 'C', 'C', 'I', 'I', 'I', 'C', 'S', 'I', 'C', 'C', 'S', 'C', 'I', 'C', 'C', 'C', 'C', 'C', 'I', 'C', 'C', 'I', 'C', 'I', 'C', 'I', 'C', 'C', 'I', 'C', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'C', 'C', 'C', 'C', 'I', 'I', 'C', 'C', 'E', 'C', 'I', 'C', 'C', 'I', 'I', 'E', 'C', 'I', 'S', 'I', 'C', 'I', 'C', 'I', 'C', 'C', 'I', 'C', 'I', 'I', 'C', 'C', 'I', 'I', 'C', 'E', 'C', 'C', 'S', 'S', 'E', 'C', 'I', 'I', 'C', 'I', 'E', 'C', 'I', 'I', 'I', 'C', 'C', 'C', 'C', 'I', 'S', 'C', 'C', 'I', 'C', 'I', 'I', 'C', 'I', 'I', 'I', 'C', 'C', 'I', 'I', 'C', 'I', 'C', 'C', 'I', 'S', 'C', 'C', 'C', 'C', 'I', 'C', 'C', 'I', 'C', 'I', 'C', 'I', 'I', 'I', 'I', 'C', 'C', 'I', 'E', 'C', 'C', 'C', 'I', 'I', 'I', 'C', 'C', 'C', 'I', 'C', 'C', 'I', 'I', 'C', 'I', 'C', 'I', 'C', 'I', 'S', 'I', 'I', 'I', 'I', 'I', 'I', 'C', 'I', 'C', 'C', 'I', 'I', 'I', 'E', 'C', 'S', 'I', 'I', 'I', 'C', 'C', 'I', 'E', 'S', 'I', 'C', 'C', 'I', 'I', 'C', 'I', 'C', 'C', 'I', 'S', 'S', 'I', 'C', 'C', 'I', 'I', 'C', 'I', 'I', 'C', 'I', 'C', 'I', 'C', 'C', 'C', 'S', 'I', 'C', 'I', 'I', 'S', 'C', 'I', 'C', 'C', 'C', 'I', 'I', 'E', 'C', 'C', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'C', 'S', 'I', 'I', 'I', 'I', 'C', 'C', 'C', 'I', 'S', 'C', 'S', 'C', 'E', 'I', 'C', 'C', 'C', 'C', 'I', 'I', 'C', 'I', 'I', 'I', 'I', 'C', 'C', 'I', 'I', 'C', 'I', 'I', 'C', 'C', 'C', 'I', 'I', 'C', 'I', 'I', 'C', 'I', 'I', 'I', 'I', 'C', 'I', 'C', 'I', 'S', 'S', 'I', 'I', 'C', 'I', 'I', 'C', 'C', 'I', 'I', 'C', 'I', 'I', 'C', 'C', 'I', 'I', 'S', 'C', 'I', 'I', 'I', 'I', 'I', 'I', 'E', 'I', 'E', 'I', 'I', 'I', 'S', 'C', 'I', 'E', 'I', 'I', 'I', 'C', 'I', 'C', 'I', 'I', 'I', 'C', 'C', 'C', 'I', 'S', 'I', 'I', 'I', 'I', 'I', 'C', 'I', 'C', 'C', 'C', 'C', 'I', 'I', 'C', 'I', 'I', 'I', 'I', 'I', 'C', 'S', 'C', 'C', 'C', 'C', 'I', 'C', 'C', 'I', 'C', 'I', 'I', 'I', 'C', 'I', 'I', 'C', 'C', 'C', 'C', 'C', 'S', 'I', 'I', 'I', 'I', 'C', 'I', 'C', 'C', 'I', 'C', 'C', 'I', 'I', 'C', 'E', 'C', 'E', 'C', 'I', 'I', 'C', 'C', 'I', 'C', 'C', 'C', 'I', 'I', 'S', 'C', 'I', 'I', 'C', 'I', 'I', 'I', 'S', 'C', 'C', 'I', 'C', 'C', 'C', 'I', 'E', 'C', 'I', 'I', 'S', 'C', 'I', 'C', 'I', 'C', 'I', 'I', 'I', 'C', 'I', 'C', 'C', 'I', 'E', 'C', 'I', 'C', 'S', 'I', 'S', 'I', 'I', 'S', 'I', 'C', 'C', 'C', 'I', 'I', 'I', 'C', 'I', 'I', 'C', 'C', 'C', 'I', 'I', 'C', 'C', 'E', 'I', 'C', 'C', 'I', 'I', 'E', 'C', 'C', 'I', 'I', 'I', 'C', 'I', 'C', 'I', 'E', 'C', 'S', 'I', 'I', 'I', 'C', 'S', 'I', 'I', 'I', 'I', 'E', 'I', 'I', 'C', 'C', 'C', 'C', 'S', 'C', 'C', 'I', 'I', 'I', 'C', 'I', 'S', 'I', 'C', 'I', 'C', 'I', 'C', 'E', 'C', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'C', 'C', 'C', 'C', 'C', 'C', 'I', 'I', 'C', 'I', 'C', 'I', 'C', 'I', 'I', 'S', 'I', 'I', 'I', 'C', 'C', 'C', 'I', 'C', 'C', 'C', 'I', 'C', 'E', 'C', 'C', 'I', 'I', 'C', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'C', 'I', 'C', 'C', 'C', 'C', 'I', 'C', 'C', 'I', 'I', 'I', 'I', 'I', 'I', 'C', 'C', 'C', 'E', 'I', 'I', 'I', 'C', 'C', 'S', 'C', 'C', 'C', 'I', 'I', 'C', 'C', 'C', 'I', 'I', 'C', 'C', 'I', 'C', 'I', 'I', 'C', 'E', 'C', 'I', 'I', 'C', 'C', 'C', 'I', 'C', 'C', 'I', 'I', 'E', 'C', 'C', 'C', 'C', 'C', 'I', 'C', 'S', 'C', 'I', 'I', 'C', 'C', 'I', 'I', 'I', 'I', 'C', 'I', 'I', 'I', 'S', 'C', 'I', 'I', 'I', 'I', 'C', 'I', 'I', 'C', 'I', 'I', 'C', 'I', 'I', 'C', 'C', 'I', 'I', 'C', 'C', 'I', 'I', 'C', 'C', 'C', 'I', 'C', 'C', 'C', 'C', 'I', 'I', 'S', 'I', 'C', 'I', 'I', 'E', 'S', 'I', 'I', 'C', 'I', 'C', 'C', 'C', 'C', 'I', 'C', 'I', 'C', 'C', 'I', 'I', 'C', 'C', 'I', 'I', 'C', 'I', 'I', 'C', 'I', 'C', 'C', 'I', 'C', 'S', 'C', 'I', 'E', 'I', 'C', 'C', 'C', 'C', 'I', 'C', 'I', 'C', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'E', 'I', 'I', 'C', 'I', 'I', 'C', 'C', 'C', 'C', 'C', 'I', 'C', 'C', 'C', 'C', 'C', 'I', 'E', 'C', 'C', 'I', 'C', 'C', 'C', 'I', 'I', 'C', 'I', 'I', 'C', 'C', 'I', 'I', 'C', 'C', 'I']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(predicted_class_labels))"
      ],
      "metadata": {
        "id": "Yr1mQDbUpLZH",
        "outputId": "6ae98ff1-1efb-40a5-b07c-fe83dfc061c2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2488\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import f1_score\n",
        "\n",
        "micro_avg_f1 = f1_score(df_examples_products_test['esci_label'], predicted_class_labels, average='micro')\n",
        "print(f\"Micro Average F1 Score: {micro_avg_f1}\")"
      ],
      "metadata": {
        "id": "IGK0iZqPpkBg",
        "outputId": "abf27ee9-197f-4652-ac85-1122a2527e7e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Micro Average F1 Score: 0.09003215434083602\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Getting predictions: Ignore for now"
      ],
      "metadata": {
        "id": "TBp5eUVD7FjV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "outputs = model(df_examples_products_test['bert_input']).logits"
      ],
      "metadata": {
        "id": "qo1Wpfd-7EzK",
        "outputId": "a04d1fbf-c747-45a7-e4a3-4416496b0279",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 860
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-24-542dbefab512>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_examples_products_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'bert_input'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/modeling_tf_utils.py\u001b[0m in \u001b[0;36mrun_call_with_unpacked_inputs\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    423\u001b[0m             \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    424\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 425\u001b[0;31m         \u001b[0munpacked_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_processing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfn_args_and_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    426\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0munpacked_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    427\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/modeling_tf_utils.py\u001b[0m in \u001b[0;36minput_processing\u001b[0;34m(func, config, **kwargs)\u001b[0m\n\u001b[1;32m    553\u001b[0m             \u001b[0moutput\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmain_input_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmain_input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    554\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 555\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m    556\u001b[0m                 \u001b[0;34mf\"Data of type {type(main_input)} is not allowed only {allowed_types} is accepted for\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    557\u001b[0m                 \u001b[0;34mf\" {main_input_name}.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Exception encountered when calling layer 'tf_bert_for_sequence_classification' (type TFBertForSequenceClassification).\n\nData of type <class 'pandas.core.series.Series'> is not allowed only (<class 'tensorflow.python.framework.tensor.Tensor'>, <class 'bool'>, <class 'int'>, <class 'transformers.utils.generic.ModelOutput'>, <class 'tuple'>, <class 'list'>, <class 'dict'>, <class 'numpy.ndarray'>) is accepted for input_ids.\n\nCall arguments received by layer 'tf_bert_for_sequence_classification' (type TFBertForSequenceClassification):\n  • input_ids=0       [input_ids, token_type_ids, attention_mask]\n16      [input_ids, token_type_ids, attention_mask]\n20      [input_ids, token_type_ids, attention_mask]\n25      [input_ids, token_type_ids, attention_mask]\n29      [input_ids, token_type_ids, attention_mask]\n                           ...                     \n9980    [input_ids, token_type_ids, attention_mask]\n9982    [input_ids, token_type_ids, attention_mask]\n9985    [input_ids, token_type_ids, attention_mask]\n9987    [input_ids, token_type_ids, attention_mask]\n9997    [input_ids, token_type_ids, attention_mask]\nName: bert_input, Length: 2488, dtype: object\n  • attention_mask=None\n  • token_type_ids=None\n  • position_ids=None\n  • head_mask=None\n  • inputs_embeds=None\n  • output_attentions=None\n  • output_hidden_states=None\n  • return_dict=None\n  • labels=None\n  • training=False"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "classifications = [model.config.id2label[output] for output in classifications]\n",
        "print(classifications)"
      ],
      "metadata": {
        "id": "mTvRSNtH7oq1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
